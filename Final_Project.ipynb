{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Muntadhar AlZayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello! This is my final project for *DTSA 5509 Introduction to Machine Learning: Supervised Learning*. I am using the data from the UCI Machine Learning Repository https://archive.ics.uci.edu/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using Bank Marketing (with social/economic context) Data Set that has the following description:\n",
    "    *This dataset is using \"in-vehicle coupon recommendation Data Set\" UCI dataset.*\n",
    "    \n",
    "With the following \n",
    "            Attribute Information:\n",
    "\n",
    "        destination: No Urgent Place, Home, Work\n",
    "        passanger: Alone, Friend(s), Kid(s), Partner (who are the passengers in the car)\n",
    "        weather: Sunny, Rainy, Snowy\n",
    "        temperature:55, 80, 30\n",
    "        time: 2PM, 10AM, 6PM, 7AM, 10PM\n",
    "        coupon: Restaurant(<$20), Coffee House, Carry out & Take away, Bar, Restaurant($20-$50)\n",
    "    expiration: 1d, 2h (the coupon expires in 1 day or in 2 hours)\n",
    "    gender: Female, Male\n",
    "    age: 21, 46, 26, 31, 41, 50plus, 36, below21\n",
    "    maritalStatus: Unmarried partner, Single, Married partner, Divorced, Widowed\n",
    "    has_Children:1, 0\n",
    "    education: Some college - no degree, Bachelors degree, Associates degree, High School Graduate, Graduate degree (Masters or Doctorate), Some High School\n",
    "    occupation: Unemployed, Architecture & Engineering, Student,\n",
    "    Education&Training&Library, Healthcare Support,\n",
    "    Healthcare Practitioners & Technical, Sales & Related, Management,\n",
    "    Arts Design Entertainment Sports & Media, Computer & Mathematical,\n",
    "    Life Physical Social Science, Personal Care & Service,\n",
    "    Community & Social Services, Office & Administrative Support,\n",
    "    Construction & Extraction, Legal, Retired,\n",
    "    Installation Maintenance & Repair, Transportation & Material Moving,\n",
    "    Business & Financial, Protective Service,\n",
    "    Food Preparation & Serving Related, Production Occupations,\n",
    "    Building & Grounds Cleaning & Maintenance, Farming Fishing & Forestry\n",
    "    income: $37500 - $49999, $62500 - $74999, $12500 - $24999, $75000 - $87499,\n",
    "    $50000 - $62499, $25000 - $37499, $100000 or More, $87500 - $99999, Less than $12500\n",
    "    Bar: never, less1, 1~3, gt8, nan4~8 (feature meaning: how many times do you go to a bar every month?)\n",
    "    CoffeeHouse: never, less1, 4~8, 1~3, gt8, nan (feature meaning: how many times do you go to a coffeehouse every month?)\n",
    "    CarryAway:n4~8, 1~3, gt8, less1, never (feature meaning: how many times do you get take-away food every month?)\n",
    "    RestaurantLessThan20: 4~8, 1~3, less1, gt8, never (feature meaning: how many times do you go to a restaurant with an average expense per person of less than $20 every month?)\n",
    "    Restaurant20To50: 1~3, less1, never, gt8, 4~8, nan (feature meaning: how many times do you go to a restaurant with average expense per person of $20 - $50 every month?)\n",
    "    toCoupon_GEQ15min:0,1 (feature meaning: driving distance to the restaurant/bar for using the coupon is greater than 15 minutes)\n",
    "    toCoupon_GEQ25min:0, 1 (feature meaning: driving distance to the restaurant/bar for using the coupon is greater than 25 minutes)\n",
    "    direction_same:0, 1 (feature meaning: whether the restaurant/bar is in the same direction as your current destination)\n",
    "    direction_opp:1, 0 (feature meaning: whether the restaurant/bar is in the same direction as your current destination)\n",
    "    Y:1, 0 (whether the coupon is accepted)\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n",
    "\n",
    "Citation: [Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Summary\n",
    "\n",
    "## Objective\n",
    "I want to use this data to help predict the conditions and personal traits for people that take coupons when they are driving past. This will help the employees of those companies to better give out those coupons and to avoid conditions where people are less likely to accept them. The data includes both personal traits, weather conditions, and range of the establishment from the place that coupons are being given out. \n",
    "\n",
    "## Main model\n",
    "The main model I will use is a Decision Tree Classifier as it utilizes all the features provided in the data, and will also help me to find the most important features. \n",
    "\n",
    "Note: While deciding when and where to give out coupons the employees will not be able to have personal trait details, but given the insight into the data we will be able to at least provide them with insight into external factors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Data\n",
    "We start with importing necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "!pip uninstall scikit-learn\n",
    "!pip install  scikit-learn==0.23.2\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Set color map to have light blue background\n",
    "sns.set()\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://archive.ics.uci.edu/ml/datasets/Bank+Marketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then import the data from the data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00603/in-vehicle-coupon-recommendation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see a sample of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be any NULL values but they are set to unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially have 25 features with 12684 rows, as we go through data cleaning this may change. Please find the description for each feature above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the columns that have null values and the course of action for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "    if(df[c].isnull().sum() / len(df) >0 and df[c].isnull().sum() / len(df) <= .05):\n",
    "        print(\"Impute column '\",c, \"'  - Percenteage of null values: \", df[c].isnull().sum() / len(df))\n",
    "    elif (df[c].isnull().sum() / len(df)) > .05:\n",
    "        print(\"Drop column '\", c, \"'  - Percenteage of null values: \", df[c].isnull().sum() / len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `car` has 99.14% missing values so it needs to be dropped as it is above the 5% acceptable null values. The rest of the columns that contain null values can be imputed with the mode of each as they are all categorical columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop column 'car'\n",
    "df = df.drop(columns=['car'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the remaining columns with null values\n",
    "cols = df.columns\n",
    "feats_w_null = []\n",
    "for c in df.columns:\n",
    "    if df[c].isnull().sum() > 0:\n",
    "        feats_w_null.append(c)\n",
    "\n",
    "\n",
    "print(feats_w_null)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each column in feats_w_null we get the mode and replace all null values with the most common entry\n",
    "\n",
    "for x in feats_w_null:\n",
    "    replacement = df[x].mode()\n",
    "    df[x].fillna(replacement[0], inplace=True)\n",
    "    print(\"Filled nulls for\", x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "    if(df[c].isnull().sum() / len(df) >0 and df[c].isnull().sum() / len(df) <= .05):\n",
    "        print(\"Impute: \",c)\n",
    "    elif (df[c].isnull().sum() / len(df)) > .05:\n",
    "        print(\"Drop :\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data cleaning, we now have no null values and 24 features with the same number of rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "Lets take a deeper dive into the categorical data and visualize them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['destination'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination = df.groupby(['destination', 'Y']).size().unstack()\n",
    "destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that there is a split between \"No Urgent Place\" and the other two categories of \"Work\" and \"Home\". This is a good indicator that the coupons are being spread at different times of the day and during different days of the week. As well, we see almost a 50/50 split within the outcome based on the value of the persons destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weather'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = df.groupby(['weather', 'Y']).size().unstack()\n",
    "weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is about a 50/50 split for people accepting the coupon based on the weather, so we can see that there is no direct correlation between the two visalized features above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### toCoupon_GEQ5min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['toCoupon_GEQ5min'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of `toCoupon_GEQ5min` are all 1 so we can drop this column as it does not provide valuable data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['toCoupon_GEQ5min'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 23 features to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical string to int\n",
    "We need to change all string categorical data into integer. I utilized the LabelEncoder function in sklearn to do this instead of manually mapping each instance to a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtypes = df.dtypes #Data Types for each column\n",
    "columns = df.columns # Columns in dataframe\n",
    "for x in columns:\n",
    "    if dtypes[x] == object: #If the values are not continous\n",
    "        print(\"Column:\",x, '\\n', \" String categories:\",df[x].unique()) #Print old string categories\n",
    "        le = preprocessing.LabelEncoder() #Init LabelEncoder\n",
    "        le.fit(df[x].unique()) #Fit it with the unique values in the dataframe column\n",
    "        df[x] = le.transform(df[x]) #Transform them into their integer values\n",
    "        print( \"  Int categories:\",df[x].unique()) #Print new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all our columns in numerical form with both continous and categorical data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Correlation\n",
    "Lets test the correlation between the data and find the most correlated feature to our result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr()) #Heatmap for correlations in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y is the last column/row in the correlation above, we can quantify it below:\n",
    "correlation = df.corr()['Y']\n",
    "print(correlation.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the most correlated feature is not correlated enough to utilize a Linear model, but lets test it with some of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(formula='Y ~ weather + coupon + CoffeeHouse ', data=df)\n",
    " \n",
    "res = model.fit() #update this value according to the result\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is a Linear Model for testing the highest correlated features, even then we don't have enough to predict future values based solely on `weather`, `coupon`, and `expiration`. So we need to use Decision Tree Classifier to have the best use of all features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we move the features and the result into x and y respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into parameters and result \n",
    "x = df.drop(columns='Y').copy()\n",
    "y = df[['Y']].copy()\n",
    "\n",
    "print(x.info())\n",
    "# print(y.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we provide a 80/20 split for trainging and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into training and testing \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data ready, we can then run random parameters on the DTC and view the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Classifier with random initial parameters \n",
    "classifier = DecisionTreeClassifier(max_depth=10, random_state=14)\n",
    "classifier.fit(x_train, y_train)\n",
    "pred = classifier.predict(x_test)\n",
    "acc_score = accuracy_score(y_true=y_test, y_pred = pred)\n",
    "\n",
    "print(\"Accuracy Score for initial DTC:\", acc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy score of 69% is nice for random initial values but we can get a better score by modifying our parameters, we can run GridSearchCV to search for the best values to give us the best Accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth' : np.arange(3,10),\n",
    "             'criterion': ['gini', 'entropy'],\n",
    "              'max_leaf_nodes' : [5,10,15,20,50,100],\n",
    "              'min_samples_split' : [2,4,5,10,15,20]\n",
    "             }\n",
    "grid_search_tree = GridSearchCV(DecisionTreeClassifier(), parameters, scoring=\"accuracy\")\n",
    "\n",
    "grid_search_tree.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best Estimator values:\", grid_search_tree.best_estimator_)\n",
    "\n",
    "print('Best Score:', np.abs(grid_search_tree.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the best parameters provided to us above, we can then use that model to predict the test data and get the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the best scoring parameters from above with test value:\n",
    "\n",
    "classifier = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
    "                       max_depth=9, max_features=None, max_leaf_nodes=100,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=15,\n",
    "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
    "                       random_state=None, splitter='best')\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "pred = classifier.predict(x_test)\n",
    "acc_score = accuracy_score(y_true=y_test, y_pred = pred)\n",
    "f_score = f1_score(y_true=y_test, y_pred=pred)\n",
    "print(\"Accuracy Score Optimized Parameters:\", acc_score)\n",
    "print(\"F1 Score Optimized Parameters:\", f_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the test data we get around the same accuracy score, now lets look at the most important features below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature importance\n",
    "features = x.columns\n",
    "scores = classifier.feature_importances_.tolist()\n",
    "res = pd.DataFrame({'features' : features, 'score': scores})\n",
    "res = res.sort_values(by=['score'], ascending=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can thus conclude that the most important identifier from the Decision Tree Classifier model is the type of coupon that is presented to the customer, the frequency the person accepting the coupon goes to a Coffe House within a month, and if the driving time to the establishment is >=25 minutes.\n",
    "\n",
    "We can see how our models accuracy will be if we choose the top 1, 2, 3 features provided above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models with top 1, 2, & 3 features\n",
    "top_1 = df[['coupon']].copy()\n",
    "top_2 = df[['coupon', 'Bar']].copy()\n",
    "top_3 = df[['coupon', 'Bar', 'income']].copy()\n",
    "\n",
    "x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(top_1,y, test_size=.2)\n",
    "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(top_2,y, test_size=.2)\n",
    "x_train_3, x_test_3, y_train_3, y_test_3 = train_test_split(top_3,y, test_size=.2)\n",
    "\n",
    "classifier.fit(x_train_1, y_train_1)\n",
    "pred1 = classifier.predict(x_test_1)\n",
    "acc_score1 = accuracy_score(y_true=y_test_1, y_pred = pred1)\n",
    "print(\"With the top (1) feature, accuracy score = \", acc_score1)\n",
    "\n",
    "classifier.fit(x_train_2, y_train_2)\n",
    "pred2 = classifier.predict(x_test_2)\n",
    "acc_score2 = accuracy_score(y_true=y_test_2, y_pred = pred2)\n",
    "print(\"With the top (2) features, accuracy score = \", acc_score2)\n",
    "\n",
    "classifier.fit(x_train_3, y_train_3)\n",
    "pred3 = classifier.predict(x_test_3)\n",
    "acc_score3 = accuracy_score(y_true=y_test_3, y_pred = pred3)\n",
    "print(\"With the top (3) features, accuracy score = \", acc_score3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy fluctuates with the addition of additional parameters. But, there is an increase in accuracy as we add more features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix \n",
    "For the optimized DTC model we created above, we want to view how we can improve the values of FN and FP. First lets view the amounts for those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "cm = sklearn.metrics.confusion_matrix(y_test, pred)\n",
    "print(cm)\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "\n",
    "y_true = y_test.values.tolist()\n",
    "pos_label_value = 1\n",
    "for l in range(len(pred)):\n",
    "    predicted = pred[l]\n",
    "    true = y_true[l][0]\n",
    "    if predicted == pos_label_value and true == pos_label_value:\n",
    "        TP += 1\n",
    "    elif predicted == pos_label_value and true != pos_label_value:\n",
    "        FP += 1\n",
    "    elif predicted != pos_label_value and true == pos_label_value:\n",
    "        FN += 1\n",
    "    elif predicted != pos_label_value and true != pos_label_value:\n",
    "        TN += 1\n",
    "print(\"TP = \", TP)\n",
    "print(\"FP = \", FP)\n",
    "print(\"TN = \", TN)\n",
    "print(\"FN = \", FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve those amounts we can change the GridSearchCV scoring to f1 instaed of accuracy to the predictions. This should help us to predict the TP and TN values a bit better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth' : np.arange(3,10),\n",
    "             'criterion': ['gini', 'entropy'],\n",
    "              'max_leaf_nodes' : [5,10,15,20,50,100],\n",
    "              'min_samples_split' : [2,4,5,10,15,20]\n",
    "             }\n",
    "grid_search_tree = GridSearchCV(DecisionTreeClassifier(), parameters, scoring=\"f1\")\n",
    "\n",
    "grid_search_tree.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best Estimator values:\", grid_search_tree.best_estimator_)\n",
    "\n",
    "print('Best Score:', np.abs(grid_search_tree.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
    "                       max_depth=7, max_features=None, max_leaf_nodes=50,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
    "                       random_state=None, splitter='best')\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "pred = classifier.predict(x_test)\n",
    "acc_score = accuracy_score(y_true=y_test, y_pred = pred)\n",
    "f_score = f1_score(y_true=y_test, y_pred=pred)\n",
    "print(\"Accuracy Score Optimized Parameters:\", acc_score)\n",
    "print(\"F1 Score Optimized Parameters:\", f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = x_train.columns\n",
    "scores = classifier.feature_importances_.tolist()\n",
    "res = pd.DataFrame({'features' : features, 'score': scores})\n",
    "res = res.sort_values(by=['score'], ascending=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "cm = sklearn.metrics.confusion_matrix(y_test, pred)\n",
    "print(cm)\n",
    "\n",
    "TP_new = 0\n",
    "FP_new = 0\n",
    "TN_new = 0\n",
    "FN_new = 0\n",
    "\n",
    "y_true = y_test.values.tolist()\n",
    "pos_label_value = 1\n",
    "for x in range(len(pred)):\n",
    "    predicted = pred[x]\n",
    "    true = y_true[x][0]\n",
    "    if predicted == pos_label_value and true == pos_label_value:\n",
    "        TP_new += 1\n",
    "    elif predicted == pos_label_value and true != pos_label_value:\n",
    "        FP_new += 1\n",
    "    elif predicted != pos_label_value and true == pos_label_value:\n",
    "        FN_new += 1\n",
    "    elif predicted != pos_label_value and true != pos_label_value:\n",
    "        TN_new += 1\n",
    "print(\"New TP = \", TP_new)\n",
    "print(\"New FP = \", FP_new)\n",
    "print(\"New TN = \", TN_new)\n",
    "print(\"New FN = \", FN_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare those values we can see the following:\n",
    "\n",
    "TP =  1136, New TP =  1159\n",
    "\n",
    "FP =  423, New FP =  467\n",
    "\n",
    "TN =  649, New TN =  605\n",
    "\n",
    "FN =  329, New FN =  306\n",
    "\n",
    "Both False Negatives and true Negatives decreased, while False Positives and True Positives increased. This may be a favorable for this model and dataset as we prefer to include more people into positive to provide them with the ability to accept the coupon despite them being a possible negative as they may be persuaded by further external factors not included in this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "For this I want to remove all personal traits, and focus on observable features. \n",
    "\n",
    "I will start with combining direction_opp and direction_same into one column with different values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_df = df.copy()\n",
    "fe_df['direction'] = 0\n",
    "fe_df.loc[((fe_df.direction_opp == 1) & (fe_df.direction_same == 0)), 'direction'] = 0 # Opposite direction\n",
    "fe_df.loc[((fe_df.direction_opp == 0) & (fe_df.direction_same == 1)), 'direction'] = 1 # Same direction\n",
    "fe_df = fe_df.drop(columns=['direction_same', 'direction_opp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will make the passenger column to be a binary, to make it more of an observable feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_df['b_passanger'] = 0\n",
    "fe_df.loc[((fe_df['passanger'] == 0), 'b_passanger')] = 0\n",
    "fe_df.loc[((fe_df['passanger'] == 1) | (fe_df['passanger'] == 2) | (fe_df['passanger'] == 3), 'b_passanger')] = 1\n",
    "fe_df = fe_df.drop(columns=['passanger'])\n",
    "fe_df = fe_df.rename(columns={'b_passanger' : 'new_passanger'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to combine the columns `toCoupon_GEQ15min` and `toCoupon_GEQ25min`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_df['toCoupon'] = 0\n",
    "fe_df.loc[((fe_df['toCoupon_GEQ15min'] == 0) & (fe_df['toCoupon_GEQ25min'] == 0), 'toCoupon')] = 0 # Less than 15 mins\n",
    "fe_df.loc[((fe_df['toCoupon_GEQ15min'] == 1) & (fe_df['toCoupon_GEQ25min'] == 0), 'toCoupon')] = 1 # between 15 and 25\n",
    "fe_df.loc[((fe_df['toCoupon_GEQ15min'] == 1) & (fe_df['toCoupon_GEQ25min'] == 1), 'toCoupon')] = 2 # Greater than 25\n",
    "fe_df = fe_df.drop(columns=['toCoupon_GEQ15min', 'toCoupon_GEQ25min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_df = fe_df[['destination', 'new_passanger', 'weather', 'temperature', 'direction', 'time', 'coupon', 'expiration', 'toCoupon', 'Y']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineered visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look into the combined columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_df['direction'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = fe_df.groupby(['direction', 'Y']).size().unstack()\n",
    "direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_df['new_passanger'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passanger = fe_df.groupby(['new_passanger', 'Y']).size().unstack()\n",
    "passanger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-run model on new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_x = fe_df.drop(columns='Y').copy()\n",
    "fe_y = fe_df[['Y']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(fe_x, fe_y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth' : np.arange(3,10),\n",
    "             'criterion': ['gini', 'entropy'],\n",
    "              'max_leaf_nodes' : [5,10,15,20,50,100],\n",
    "              'min_samples_split' : [2,4,5,10,15,20]\n",
    "             }\n",
    "grid_search_tree = GridSearchCV(DecisionTreeClassifier(), parameters, scoring=\"accuracy\")\n",
    "\n",
    "grid_search_tree.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best Estimator values:\", grid_search_tree.best_estimator_)\n",
    "\n",
    "print('Best Score:', np.abs(grid_search_tree.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
    "                       max_depth=9, max_features=None, max_leaf_nodes=100,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
    "                       random_state=None, splitter='best')\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "pred = classifier.predict(x_test)\n",
    "acc_score = accuracy_score(y_true=y_test, y_pred = pred)\n",
    "f_score = f1_score(y_true=y_test, y_pred=pred)\n",
    "print(\"Accuracy Score Optimized Parameters:\", acc_score)\n",
    "print(\"F1 Score Optimized Parameters:\", f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = x_train.columns\n",
    "scores = classifier.feature_importances_.tolist()\n",
    "res = pd.DataFrame({'features' : features, 'score': scores})\n",
    "res = res.sort_values(by=['score'], ascending=False)\n",
    "print(\"Feature importance\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sklearn.metrics.confusion_matrix(y_test, pred)\n",
    "print(cm)\n",
    "\n",
    "fe_TP = 0\n",
    "fe_FP = 0\n",
    "fe_TN = 0\n",
    "fe_FN = 0\n",
    "\n",
    "y_true = y_test.values.tolist()\n",
    "pos_label_value = 1\n",
    "for x in range(len(pred)):\n",
    "    predicted = pred[x]\n",
    "    true = y_true[x][0]\n",
    "    if predicted == pos_label_value and true == pos_label_value:\n",
    "        fe_TP += 1\n",
    "    elif predicted == pos_label_value and true != pos_label_value:\n",
    "        fe_FP += 1\n",
    "    elif predicted != pos_label_value and true == pos_label_value:\n",
    "        fe_FN += 1\n",
    "    elif predicted != pos_label_value and true != pos_label_value:\n",
    "        fe_TN += 1\n",
    "print(\"With max accuracy we get:\")\n",
    "print(\"FE Acc TP = \", fe_TP)\n",
    "print(\"FE Acc FP = \", fe_FP)\n",
    "print(\"FE Acc TN = \", fe_TN)\n",
    "print(\"FE Acc FN = \", fe_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth' : np.arange(3,10),\n",
    "             'criterion': ['gini', 'entropy'],\n",
    "              'max_leaf_nodes' : [5,10,15,20,50,100],\n",
    "              'min_samples_split' : [2,4,5,10,15,20]\n",
    "             }\n",
    "grid_search_tree = GridSearchCV(DecisionTreeClassifier(), parameters, scoring=\"f1\")\n",
    "\n",
    "grid_search_tree.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best Estimator values:\", grid_search_tree.best_estimator_)\n",
    "\n",
    "print('Best Score:', np.abs(grid_search_tree.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
    "                       max_depth=6, max_features=None, max_leaf_nodes=50,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
    "                       random_state=None, splitter='best')\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "pred = classifier.predict(x_test)\n",
    "acc_score = accuracy_score(y_true=y_test, y_pred = pred)\n",
    "f_score = f1_score(y_true=y_test, y_pred=pred)\n",
    "print(\"Accuracy Score Optimized Parameters:\", acc_score)\n",
    "print(\"F1 Score Optimized Parameters:\", f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = x_train.columns\n",
    "scores = classifier.feature_importances_.tolist()\n",
    "res = pd.DataFrame({'features' : features, 'score': scores})\n",
    "res = res.sort_values(by=['score'], ascending=False)\n",
    "print(\"Feature importance\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sklearn.metrics.confusion_matrix(y_test, pred)\n",
    "print(cm)\n",
    "\n",
    "fe_TP = 0\n",
    "fe_FP = 0\n",
    "fe_TN = 0\n",
    "fe_FN = 0\n",
    "\n",
    "y_true = y_test.values.tolist()\n",
    "pos_label_value = 1\n",
    "for x in range(len(pred)):\n",
    "    predicted = pred[x]\n",
    "    true = y_true[x][0]\n",
    "    if predicted == pos_label_value and true == pos_label_value:\n",
    "        fe_TP += 1\n",
    "    elif predicted == pos_label_value and true != pos_label_value:\n",
    "        fe_FP += 1\n",
    "    elif predicted != pos_label_value and true == pos_label_value:\n",
    "        fe_FN += 1\n",
    "    elif predicted != pos_label_value and true != pos_label_value:\n",
    "        fe_TN += 1\n",
    "print(\"With max f1 we get:\")\n",
    "print(\"FE f1 TP = \", fe_TP)\n",
    "print(\"FE f1 FP = \", fe_FP)\n",
    "print(\"FE f1 TN = \", fe_TN)\n",
    "print(\"FE f1 FN = \", fe_FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we only ran our model on observable features, we can see that the model improved, and will help the people passing out the coupons to use their observation skills, and decision for time and direction to pass out the coupons more effectively to people that are more likely to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model before feature engineering may not be the best even after the changes made to be more focused on f1-score rather than accuracy. But, having more positives than negatives in this situation is not the worse as it would lead to more wasted time of offering coupons to those that may not accept them, but it also will lead to more true positive outcomes. \n",
    "\n",
    "After choosing only the obersable features we got a worse model, but unless we have a camera that views into the car and gives us the backstory and details for each person it will not be a practical model. This way we can provide those employees with the insight into what factors that they can observe to offer a coupon to a driver that is more likely to accept a coupon and improve their effeciency. \n",
    "\n",
    "To improve this we might need to choose a different model that uses weights on each feature to have better insight into which feature will impact the customer not accepting more. I would recommend a bayesian model as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
